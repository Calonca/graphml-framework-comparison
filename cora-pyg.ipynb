{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 231,
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch_geometric.utils import softmax\n",
    "import torch_geometric.utils.convert as conv\n",
    "import torch\n",
    "from torch_geometric.nn import GCNConv, GATConv,SAGEConv\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "# load the graph from the graphml file\n",
    "def load_graph(file_path):\n",
    "    G = nx.read_graphml(file_path)\n",
    "    return G\n",
    "\n",
    "file_path = 'data/cora.graphml'\n",
    "# define the classification label\n",
    "classification_label = 'subject'\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "G = load_graph(file_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "#Removing not useful attributes\n",
    "for id in G.nodes:\n",
    "    del G.nodes[id]['label']\n",
    "for s,d in G.edges:\n",
    "    del G.edges[s,d]['label']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph with 2708 nodes and 5278 edges <class 'networkx.classes.graph.Graph'>\n",
      "('1033', {'w_0': 0, 'w_1': 0, 'w_2': 0, 'w_3': 0, 'w_4': 0, 'w_5': 0, 'w_6': 0, 'w_7': 0, 'w_8': 0, 'w_9': 1, 'w_10': 0, 'w_11': 0, 'w_12': 0, 'w_13': 0, 'w_14': 0, 'w_15': 0, 'w_16': 0, 'w_17': 0, 'w_18': 0, 'w_19': 0, 'w_20': 0, 'w_21': 0, 'w_22': 0, 'w_23': 0, 'w_24': 0, 'w_25': 0, 'w_26': 0, 'w_27': 0, 'w_28': 0, 'w_29': 0, 'w_30': 0, 'w_31': 0, 'w_32': 0, 'w_33': 0, 'w_34': 0, 'w_35': 0, 'w_36': 0, 'w_37': 0, 'w_38': 0, 'w_39': 0, 'w_40': 0, 'w_41': 0, 'w_42': 0, 'w_43': 0, 'w_44': 0, 'w_45': 0, 'w_46': 0, 'w_47': 0, 'w_48': 0, 'w_49': 0, 'w_50': 0, 'w_51': 0, 'w_52': 0, 'w_53': 0, 'w_54': 0, 'w_55': 0, 'w_56': 0, 'w_57': 0, 'w_58': 0, 'w_59': 0, 'w_60': 0, 'w_61': 0, 'w_62': 0, 'w_63': 0, 'w_64': 0, 'w_65': 0, 'w_66': 0, 'w_67': 0, 'w_68': 0, 'w_69': 0, 'w_70': 0, 'w_71': 0, 'w_72': 0, 'w_73': 0, 'w_74': 0, 'w_75': 0, 'w_76': 0, 'w_77': 0, 'w_78': 0, 'w_79': 0, 'w_80': 0, 'w_81': 0, 'w_82': 0, 'w_83': 0, 'w_84': 0, 'w_85': 0, 'w_86': 0, 'w_87': 0, 'w_88': 0, 'w_89': 0, 'w_90': 0, 'w_91': 0, 'w_92': 0, 'w_93': 0, 'w_94': 0, 'w_95': 0, 'w_96': 0, 'w_97': 1, 'w_98': 0, 'w_99': 0, 'w_100': 0, 'w_101': 0, 'w_102': 0, 'w_103': 0, 'w_104': 0, 'w_105': 0, 'w_106': 0, 'w_107': 0, 'w_108': 0, 'w_109': 0, 'w_110': 0, 'w_111': 0, 'w_112': 0, 'w_113': 0, 'w_114': 0, 'w_115': 0, 'w_116': 0, 'w_117': 0, 'w_118': 0, 'w_119': 0, 'w_120': 0, 'w_121': 0, 'w_122': 0, 'w_123': 0, 'w_124': 0, 'w_125': 0, 'w_126': 0, 'w_127': 0, 'w_128': 0, 'w_129': 0, 'w_130': 0, 'w_131': 0, 'w_132': 0, 'w_133': 0, 'w_134': 0, 'w_135': 0, 'w_136': 0, 'w_137': 0, 'w_138': 0, 'w_139': 0, 'w_140': 1, 'w_141': 0, 'w_142': 0, 'w_143': 0, 'w_144': 0, 'w_145': 0, 'w_146': 0, 'w_147': 0, 'w_148': 0, 'w_149': 0, 'w_150': 0, 'w_151': 0, 'w_152': 0, 'w_153': 0, 'w_154': 0, 'w_155': 0, 'w_156': 0, 'w_157': 0, 'w_158': 0, 'w_159': 0, 'w_160': 0, 'w_161': 0, 'w_162': 0, 'w_163': 0, 'w_164': 0, 'w_165': 0, 'w_166': 0, 'w_167': 0, 'w_168': 0, 'w_169': 0, 'w_170': 0, 'w_171': 0, 'w_172': 0, 'w_173': 0, 'w_174': 0, 'w_175': 0, 'w_176': 0, 'w_177': 0, 'w_178': 0, 'w_179': 0, 'w_180': 0, 'w_181': 0, 'w_182': 0, 'w_183': 0, 'w_184': 0, 'w_185': 0, 'w_186': 0, 'w_187': 0, 'w_188': 0, 'w_189': 0, 'w_190': 0, 'w_191': 0, 'w_192': 0, 'w_193': 0, 'w_194': 0, 'w_195': 0, 'w_196': 0, 'w_197': 0, 'w_198': 0, 'w_199': 0, 'w_200': 0, 'w_201': 0, 'w_202': 0, 'w_203': 0, 'w_204': 0, 'w_205': 0, 'w_206': 0, 'w_207': 0, 'w_208': 0, 'w_209': 0, 'w_210': 0, 'w_211': 0, 'w_212': 0, 'w_213': 0, 'w_214': 0, 'w_215': 0, 'w_216': 0, 'w_217': 0, 'w_218': 0, 'w_219': 0, 'w_220': 0, 'w_221': 0, 'w_222': 0, 'w_223': 0, 'w_224': 0, 'w_225': 0, 'w_226': 0, 'w_227': 0, 'w_228': 0, 'w_229': 0, 'w_230': 0, 'w_231': 0, 'w_232': 0, 'w_233': 0, 'w_234': 0, 'w_235': 0, 'w_236': 0, 'w_237': 0, 'w_238': 0, 'w_239': 0, 'w_240': 0, 'w_241': 0, 'w_242': 0, 'w_243': 0, 'w_244': 0, 'w_245': 0, 'w_246': 0, 'w_247': 0, 'w_248': 0, 'w_249': 0, 'w_250': 0, 'w_251': 0, 'w_252': 0, 'w_253': 0, 'w_254': 0, 'w_255': 0, 'w_256': 0, 'w_257': 0, 'w_258': 0, 'w_259': 0, 'w_260': 0, 'w_261': 0, 'w_262': 0, 'w_263': 0, 'w_264': 0, 'w_265': 0, 'w_266': 0, 'w_267': 0, 'w_268': 0, 'w_269': 0, 'w_270': 0, 'w_271': 0, 'w_272': 0, 'w_273': 0, 'w_274': 0, 'w_275': 0, 'w_276': 0, 'w_277': 0, 'w_278': 0, 'w_279': 0, 'w_280': 0, 'w_281': 0, 'w_282': 0, 'w_283': 0, 'w_284': 0, 'w_285': 0, 'w_286': 0, 'w_287': 0, 'w_288': 0, 'w_289': 0, 'w_290': 0, 'w_291': 0, 'w_292': 0, 'w_293': 0, 'w_294': 0, 'w_295': 0, 'w_296': 0, 'w_297': 0, 'w_298': 0, 'w_299': 1, 'w_300': 0, 'w_301': 0, 'w_302': 0, 'w_303': 0, 'w_304': 0, 'w_305': 0, 'w_306': 0, 'w_307': 0, 'w_308': 0, 'w_309': 0, 'w_310': 1, 'w_311': 0, 'w_312': 0, 'w_313': 0, 'w_314': 0, 'w_315': 0, 'w_316': 0, 'w_317': 0, 'w_318': 0, 'w_319': 0, 'w_320': 0, 'w_321': 0, 'w_322': 0, 'w_323': 0, 'w_324': 0, 'w_325': 0, 'w_326': 0, 'w_327': 0, 'w_328': 0, 'w_329': 0, 'w_330': 0, 'w_331': 0, 'w_332': 0, 'w_333': 0, 'w_334': 0, 'w_335': 0, 'w_336': 0, 'w_337': 0, 'w_338': 0, 'w_339': 0, 'w_340': 0, 'w_341': 0, 'w_342': 0, 'w_343': 0, 'w_344': 0, 'w_345': 0, 'w_346': 0, 'w_347': 0, 'w_348': 0, 'w_349': 0, 'w_350': 0, 'w_351': 0, 'w_352': 0, 'w_353': 0, 'w_354': 0, 'w_355': 0, 'w_356': 0, 'w_357': 0, 'w_358': 0, 'w_359': 0, 'w_360': 1, 'w_361': 0, 'w_362': 0, 'w_363': 0, 'w_364': 0, 'w_365': 0, 'w_366': 0, 'w_367': 0, 'w_368': 0, 'w_369': 0, 'w_370': 0, 'w_371': 0, 'w_372': 0, 'w_373': 0, 'w_374': 0, 'w_375': 1, 'w_376': 0, 'w_377': 0, 'w_378': 1, 'w_379': 0, 'w_380': 0, 'w_381': 0, 'w_382': 0, 'w_383': 0, 'w_384': 0, 'w_385': 0, 'w_386': 0, 'w_387': 0, 'w_388': 0, 'w_389': 0, 'w_390': 0, 'w_391': 0, 'w_392': 0, 'w_393': 0, 'w_394': 0, 'w_395': 0, 'w_396': 0, 'w_397': 0, 'w_398': 0, 'w_399': 0, 'w_400': 0, 'w_401': 0, 'w_402': 0, 'w_403': 0, 'w_404': 0, 'w_405': 0, 'w_406': 1, 'w_407': 0, 'w_408': 0, 'w_409': 0, 'w_410': 0, 'w_411': 0, 'w_412': 0, 'w_413': 0, 'w_414': 0, 'w_415': 0, 'w_416': 0, 'w_417': 0, 'w_418': 0, 'w_419': 0, 'w_420': 0, 'w_421': 0, 'w_422': 0, 'w_423': 0, 'w_424': 0, 'w_425': 0, 'w_426': 0, 'w_427': 0, 'w_428': 0, 'w_429': 0, 'w_430': 0, 'w_431': 0, 'w_432': 0, 'w_433': 0, 'w_434': 0, 'w_435': 0, 'w_436': 0, 'w_437': 0, 'w_438': 0, 'w_439': 0, 'w_440': 0, 'w_441': 0, 'w_442': 0, 'w_443': 0, 'w_444': 0, 'w_445': 0, 'w_446': 0, 'w_447': 0, 'w_448': 0, 'w_449': 0, 'w_450': 0, 'w_451': 0, 'w_452': 0, 'w_453': 0, 'w_454': 0, 'w_455': 0, 'w_456': 0, 'w_457': 0, 'w_458': 0, 'w_459': 0, 'w_460': 0, 'w_461': 0, 'w_462': 0, 'w_463': 0, 'w_464': 0, 'w_465': 0, 'w_466': 0, 'w_467': 0, 'w_468': 0, 'w_469': 0, 'w_470': 0, 'w_471': 0, 'w_472': 0, 'w_473': 0, 'w_474': 0, 'w_475': 0, 'w_476': 0, 'w_477': 0, 'w_478': 0, 'w_479': 0, 'w_480': 0, 'w_481': 0, 'w_482': 0, 'w_483': 0, 'w_484': 0, 'w_485': 0, 'w_486': 0, 'w_487': 0, 'w_488': 0, 'w_489': 0, 'w_490': 0, 'w_491': 0, 'w_492': 0, 'w_493': 0, 'w_494': 0, 'w_495': 1, 'w_496': 0, 'w_497': 0, 'w_498': 0, 'w_499': 0, 'w_500': 0, 'w_501': 0, 'w_502': 0, 'w_503': 0, 'w_504': 0, 'w_505': 0, 'w_506': 0, 'w_507': 0, 'w_508': 0, 'w_509': 0, 'w_510': 0, 'w_511': 0, 'w_512': 0, 'w_513': 0, 'w_514': 0, 'w_515': 0, 'w_516': 0, 'w_517': 0, 'w_518': 0, 'w_519': 0, 'w_520': 0, 'w_521': 0, 'w_522': 0, 'w_523': 0, 'w_524': 0, 'w_525': 0, 'w_526': 0, 'w_527': 0, 'w_528': 0, 'w_529': 0, 'w_530': 0, 'w_531': 0, 'w_532': 0, 'w_533': 0, 'w_534': 0, 'w_535': 0, 'w_536': 0, 'w_537': 0, 'w_538': 0, 'w_539': 0, 'w_540': 0, 'w_541': 0, 'w_542': 0, 'w_543': 0, 'w_544': 0, 'w_545': 0, 'w_546': 0, 'w_547': 0, 'w_548': 0, 'w_549': 0, 'w_550': 0, 'w_551': 0, 'w_552': 0, 'w_553': 0, 'w_554': 0, 'w_555': 0, 'w_556': 0, 'w_557': 0, 'w_558': 0, 'w_559': 0, 'w_560': 0, 'w_561': 0, 'w_562': 0, 'w_563': 0, 'w_564': 0, 'w_565': 0, 'w_566': 0, 'w_567': 0, 'w_568': 0, 'w_569': 0, 'w_570': 0, 'w_571': 0, 'w_572': 0, 'w_573': 0, 'w_574': 0, 'w_575': 0, 'w_576': 0, 'w_577': 0, 'w_578': 0, 'w_579': 0, 'w_580': 0, 'w_581': 0, 'w_582': 0, 'w_583': 0, 'w_584': 0, 'w_585': 0, 'w_586': 0, 'w_587': 0, 'w_588': 0, 'w_589': 0, 'w_590': 0, 'w_591': 0, 'w_592': 0, 'w_593': 0, 'w_594': 0, 'w_595': 0, 'w_596': 0, 'w_597': 0, 'w_598': 0, 'w_599': 0, 'w_600': 0, 'w_601': 0, 'w_602': 0, 'w_603': 0, 'w_604': 0, 'w_605': 0, 'w_606': 0, 'w_607': 0, 'w_608': 0, 'w_609': 0, 'w_610': 0, 'w_611': 0, 'w_612': 0, 'w_613': 0, 'w_614': 0, 'w_615': 0, 'w_616': 0, 'w_617': 0, 'w_618': 0, 'w_619': 0, 'w_620': 0, 'w_621': 0, 'w_622': 0, 'w_623': 0, 'w_624': 0, 'w_625': 1, 'w_626': 0, 'w_627': 0, 'w_628': 0, 'w_629': 0, 'w_630': 0, 'w_631': 0, 'w_632': 0, 'w_633': 0, 'w_634': 0, 'w_635': 0, 'w_636': 0, 'w_637': 0, 'w_638': 0, 'w_639': 0, 'w_640': 0, 'w_641': 0, 'w_642': 0, 'w_643': 0, 'w_644': 0, 'w_645': 0, 'w_646': 0, 'w_647': 0, 'w_648': 0, 'w_649': 0, 'w_650': 0, 'w_651': 0, 'w_652': 0, 'w_653': 0, 'w_654': 0, 'w_655': 0, 'w_656': 0, 'w_657': 0, 'w_658': 0, 'w_659': 0, 'w_660': 0, 'w_661': 0, 'w_662': 0, 'w_663': 0, 'w_664': 0, 'w_665': 0, 'w_666': 0, 'w_667': 0, 'w_668': 0, 'w_669': 0, 'w_670': 0, 'w_671': 0, 'w_672': 0, 'w_673': 0, 'w_674': 0, 'w_675': 0, 'w_676': 0, 'w_677': 0, 'w_678': 0, 'w_679': 0, 'w_680': 0, 'w_681': 0, 'w_682': 0, 'w_683': 0, 'w_684': 0, 'w_685': 0, 'w_686': 0, 'w_687': 0, 'w_688': 0, 'w_689': 0, 'w_690': 0, 'w_691': 0, 'w_692': 0, 'w_693': 0, 'w_694': 0, 'w_695': 0, 'w_696': 0, 'w_697': 0, 'w_698': 0, 'w_699': 0, 'w_700': 0, 'w_701': 0, 'w_702': 0, 'w_703': 0, 'w_704': 0, 'w_705': 0, 'w_706': 0, 'w_707': 0, 'w_708': 0, 'w_709': 0, 'w_710': 0, 'w_711': 0, 'w_712': 0, 'w_713': 0, 'w_714': 0, 'w_715': 0, 'w_716': 0, 'w_717': 0, 'w_718': 0, 'w_719': 0, 'w_720': 0, 'w_721': 0, 'w_722': 0, 'w_723': 1, 'w_724': 0, 'w_725': 0, 'w_726': 0, 'w_727': 0, 'w_728': 0, 'w_729': 0, 'w_730': 0, 'w_731': 0, 'w_732': 0, 'w_733': 0, 'w_734': 0, 'w_735': 0, 'w_736': 0, 'w_737': 0, 'w_738': 0, 'w_739': 0, 'w_740': 0, 'w_741': 0, 'w_742': 0, 'w_743': 0, 'w_744': 0, 'w_745': 0, 'w_746': 0, 'w_747': 0, 'w_748': 0, 'w_749': 0, 'w_750': 0, 'w_751': 0, 'w_752': 0, 'w_753': 0, 'w_754': 0, 'w_755': 0, 'w_756': 0, 'w_757': 0, 'w_758': 0, 'w_759': 0, 'w_760': 0, 'w_761': 0, 'w_762': 0, 'w_763': 0, 'w_764': 0, 'w_765': 0, 'w_766': 0, 'w_767': 0, 'w_768': 0, 'w_769': 0, 'w_770': 0, 'w_771': 0, 'w_772': 0, 'w_773': 0, 'w_774': 0, 'w_775': 0, 'w_776': 0, 'w_777': 0, 'w_778': 0, 'w_779': 0, 'w_780': 0, 'w_781': 0, 'w_782': 0, 'w_783': 0, 'w_784': 0, 'w_785': 0, 'w_786': 0, 'w_787': 0, 'w_788': 0, 'w_789': 0, 'w_790': 0, 'w_791': 0, 'w_792': 0, 'w_793': 0, 'w_794': 0, 'w_795': 0, 'w_796': 0, 'w_797': 0, 'w_798': 0, 'w_799': 0, 'w_800': 0, 'w_801': 0, 'w_802': 0, 'w_803': 0, 'w_804': 0, 'w_805': 0, 'w_806': 0, 'w_807': 0, 'w_808': 0, 'w_809': 0, 'w_810': 0, 'w_811': 0, 'w_812': 0, 'w_813': 0, 'w_814': 0, 'w_815': 0, 'w_816': 0, 'w_817': 0, 'w_818': 0, 'w_819': 0, 'w_820': 0, 'w_821': 0, 'w_822': 0, 'w_823': 0, 'w_824': 0, 'w_825': 0, 'w_826': 0, 'w_827': 0, 'w_828': 0, 'w_829': 0, 'w_830': 0, 'w_831': 0, 'w_832': 0, 'w_833': 0, 'w_834': 0, 'w_835': 0, 'w_836': 0, 'w_837': 0, 'w_838': 0, 'w_839': 0, 'w_840': 0, 'w_841': 0, 'w_842': 0, 'w_843': 0, 'w_844': 0, 'w_845': 0, 'w_846': 0, 'w_847': 0, 'w_848': 0, 'w_849': 0, 'w_850': 0, 'w_851': 0, 'w_852': 0, 'w_853': 0, 'w_854': 0, 'w_855': 0, 'w_856': 0, 'w_857': 0, 'w_858': 0, 'w_859': 0, 'w_860': 0, 'w_861': 0, 'w_862': 0, 'w_863': 0, 'w_864': 0, 'w_865': 0, 'w_866': 0, 'w_867': 0, 'w_868': 0, 'w_869': 0, 'w_870': 0, 'w_871': 0, 'w_872': 0, 'w_873': 0, 'w_874': 0, 'w_875': 0, 'w_876': 0, 'w_877': 0, 'w_878': 0, 'w_879': 0, 'w_880': 0, 'w_881': 0, 'w_882': 0, 'w_883': 0, 'w_884': 0, 'w_885': 0, 'w_886': 0, 'w_887': 0, 'w_888': 0, 'w_889': 0, 'w_890': 0, 'w_891': 0, 'w_892': 0, 'w_893': 0, 'w_894': 0, 'w_895': 0, 'w_896': 0, 'w_897': 0, 'w_898': 0, 'w_899': 0, 'w_900': 0, 'w_901': 0, 'w_902': 0, 'w_903': 0, 'w_904': 0, 'w_905': 0, 'w_906': 0, 'w_907': 0, 'w_908': 0, 'w_909': 0, 'w_910': 0, 'w_911': 1, 'w_912': 0, 'w_913': 0, 'w_914': 0, 'w_915': 0, 'w_916': 0, 'w_917': 0, 'w_918': 0, 'w_919': 0, 'w_920': 0, 'w_921': 0, 'w_922': 0, 'w_923': 0, 'w_924': 0, 'w_925': 0, 'w_926': 0, 'w_927': 0, 'w_928': 0, 'w_929': 0, 'w_930': 0, 'w_931': 0, 'w_932': 0, 'w_933': 0, 'w_934': 0, 'w_935': 0, 'w_936': 0, 'w_937': 0, 'w_938': 0, 'w_939': 0, 'w_940': 0, 'w_941': 0, 'w_942': 0, 'w_943': 0, 'w_944': 0, 'w_945': 0, 'w_946': 0, 'w_947': 0, 'w_948': 0, 'w_949': 0, 'w_950': 0, 'w_951': 0, 'w_952': 0, 'w_953': 0, 'w_954': 0, 'w_955': 0, 'w_956': 0, 'w_957': 0, 'w_958': 0, 'w_959': 0, 'w_960': 0, 'w_961': 0, 'w_962': 0, 'w_963': 1, 'w_964': 0, 'w_965': 0, 'w_966': 0, 'w_967': 0, 'w_968': 0, 'w_969': 0, 'w_970': 0, 'w_971': 0, 'w_972': 0, 'w_973': 1, 'w_974': 0, 'w_975': 0, 'w_976': 0, 'w_977': 0, 'w_978': 0, 'w_979': 0, 'w_980': 0, 'w_981': 0, 'w_982': 0, 'w_983': 0, 'w_984': 0, 'w_985': 0, 'w_986': 0, 'w_987': 0, 'w_988': 0, 'w_989': 0, 'w_990': 1, 'w_991': 0, 'w_992': 0, 'w_993': 0, 'w_994': 0, 'w_995': 0, 'w_996': 0, 'w_997': 0, 'w_998': 0, 'w_999': 0, 'w_1000': 0, 'w_1001': 0, 'w_1002': 0, 'w_1003': 0, 'w_1004': 0, 'w_1005': 0, 'w_1006': 0, 'w_1007': 0, 'w_1008': 0, 'w_1009': 0, 'w_1010': 0, 'w_1011': 0, 'w_1012': 0, 'w_1013': 0, 'w_1014': 0, 'w_1015': 0, 'w_1016': 0, 'w_1017': 0, 'w_1018': 0, 'w_1019': 0, 'w_1020': 0, 'w_1021': 0, 'w_1022': 0, 'w_1023': 0, 'w_1024': 0, 'w_1025': 0, 'w_1026': 0, 'w_1027': 0, 'w_1028': 0, 'w_1029': 0, 'w_1030': 0, 'w_1031': 0, 'w_1032': 0, 'w_1033': 0, 'w_1034': 0, 'w_1035': 0, 'w_1036': 0, 'w_1037': 0, 'w_1038': 0, 'w_1039': 0, 'w_1040': 0, 'w_1041': 0, 'w_1042': 0, 'w_1043': 0, 'w_1044': 0, 'w_1045': 0, 'w_1046': 0, 'w_1047': 0, 'w_1048': 0, 'w_1049': 0, 'w_1050': 0, 'w_1051': 0, 'w_1052': 0, 'w_1053': 0, 'w_1054': 0, 'w_1055': 0, 'w_1056': 0, 'w_1057': 0, 'w_1058': 0, 'w_1059': 0, 'w_1060': 0, 'w_1061': 0, 'w_1062': 0, 'w_1063': 0, 'w_1064': 0, 'w_1065': 0, 'w_1066': 0, 'w_1067': 0, 'w_1068': 0, 'w_1069': 0, 'w_1070': 0, 'w_1071': 1, 'w_1072': 0, 'w_1073': 0, 'w_1074': 0, 'w_1075': 0, 'w_1076': 0, 'w_1077': 0, 'w_1078': 0, 'w_1079': 0, 'w_1080': 0, 'w_1081': 0, 'w_1082': 0, 'w_1083': 0, 'w_1084': 0, 'w_1085': 0, 'w_1086': 0, 'w_1087': 0, 'w_1088': 0, 'w_1089': 0, 'w_1090': 0, 'w_1091': 0, 'w_1092': 0, 'w_1093': 0, 'w_1094': 0, 'w_1095': 0, 'w_1096': 0, 'w_1097': 0, 'w_1098': 0, 'w_1099': 0, 'w_1100': 0, 'w_1101': 0, 'w_1102': 0, 'w_1103': 0, 'w_1104': 0, 'w_1105': 0, 'w_1106': 0, 'w_1107': 0, 'w_1108': 0, 'w_1109': 0, 'w_1110': 0, 'w_1111': 0, 'w_1112': 0, 'w_1113': 0, 'w_1114': 0, 'w_1115': 0, 'w_1116': 0, 'w_1117': 0, 'w_1118': 0, 'w_1119': 0, 'w_1120': 0, 'w_1121': 0, 'w_1122': 0, 'w_1123': 0, 'w_1124': 0, 'w_1125': 0, 'w_1126': 0, 'w_1127': 0, 'w_1128': 0, 'w_1129': 0, 'w_1130': 0, 'w_1131': 0, 'w_1132': 0, 'w_1133': 0, 'w_1134': 0, 'w_1135': 0, 'w_1136': 0, 'w_1137': 0, 'w_1138': 0, 'w_1139': 0, 'w_1140': 0, 'w_1141': 0, 'w_1142': 0, 'w_1143': 0, 'w_1144': 0, 'w_1145': 0, 'w_1146': 0, 'w_1147': 0, 'w_1148': 0, 'w_1149': 0, 'w_1150': 0, 'w_1151': 0, 'w_1152': 0, 'w_1153': 0, 'w_1154': 0, 'w_1155': 0, 'w_1156': 0, 'w_1157': 0, 'w_1158': 0, 'w_1159': 0, 'w_1160': 0, 'w_1161': 0, 'w_1162': 0, 'w_1163': 0, 'w_1164': 0, 'w_1165': 0, 'w_1166': 0, 'w_1167': 0, 'w_1168': 0, 'w_1169': 0, 'w_1170': 0, 'w_1171': 0, 'w_1172': 0, 'w_1173': 0, 'w_1174': 0, 'w_1175': 0, 'w_1176': 0, 'w_1177': 1, 'w_1178': 0, 'w_1179': 0, 'w_1180': 0, 'w_1181': 0, 'w_1182': 0, 'w_1183': 0, 'w_1184': 0, 'w_1185': 0, 'w_1186': 0, 'w_1187': 0, 'w_1188': 0, 'w_1189': 0, 'w_1190': 0, 'w_1191': 0, 'w_1192': 1, 'w_1193': 0, 'w_1194': 0, 'w_1195': 0, 'w_1196': 0, 'w_1197': 0, 'w_1198': 0, 'w_1199': 0, 'w_1200': 0, 'w_1201': 0, 'w_1202': 0, 'w_1203': 0, 'w_1204': 0, 'w_1205': 0, 'w_1206': 0, 'w_1207': 0, 'w_1208': 0, 'w_1209': 0, 'w_1210': 0, 'w_1211': 0, 'w_1212': 0, 'w_1213': 0, 'w_1214': 0, 'w_1215': 0, 'w_1216': 0, 'w_1217': 0, 'w_1218': 0, 'w_1219': 0, 'w_1220': 0, 'w_1221': 0, 'w_1222': 0, 'w_1223': 0, 'w_1224': 0, 'w_1225': 0, 'w_1226': 0, 'w_1227': 0, 'w_1228': 0, 'w_1229': 0, 'w_1230': 0, 'w_1231': 1, 'w_1232': 0, 'w_1233': 0, 'w_1234': 0, 'w_1235': 0, 'w_1236': 0, 'w_1237': 0, 'w_1238': 0, 'w_1239': 0, 'w_1240': 0, 'w_1241': 0, 'w_1242': 0, 'w_1243': 0, 'w_1244': 0, 'w_1245': 0, 'w_1246': 0, 'w_1247': 0, 'w_1248': 0, 'w_1249': 0, 'w_1250': 0, 'w_1251': 0, 'w_1252': 0, 'w_1253': 0, 'w_1254': 0, 'w_1255': 0, 'w_1256': 0, 'w_1257': 0, 'w_1258': 0, 'w_1259': 0, 'w_1260': 0, 'w_1261': 0, 'w_1262': 0, 'w_1263': 1, 'w_1264': 0, 'w_1265': 0, 'w_1266': 0, 'w_1267': 0, 'w_1268': 0, 'w_1269': 0, 'w_1270': 0, 'w_1271': 0, 'w_1272': 0, 'w_1273': 0, 'w_1274': 0, 'w_1275': 0, 'w_1276': 0, 'w_1277': 0, 'w_1278': 0, 'w_1279': 0, 'w_1280': 0, 'w_1281': 0, 'w_1282': 0, 'w_1283': 0, 'w_1284': 0, 'w_1285': 0, 'w_1286': 0, 'w_1287': 0, 'w_1288': 0, 'w_1289': 0, 'w_1290': 0, 'w_1291': 0, 'w_1292': 0, 'w_1293': 0, 'w_1294': 0, 'w_1295': 0, 'w_1296': 0, 'w_1297': 0, 'w_1298': 1, 'w_1299': 0, 'w_1300': 0, 'w_1301': 0, 'w_1302': 0, 'w_1303': 0, 'w_1304': 0, 'w_1305': 0, 'w_1306': 0, 'w_1307': 0, 'w_1308': 0, 'w_1309': 0, 'w_1310': 0, 'w_1311': 0, 'w_1312': 0, 'w_1313': 0, 'w_1314': 0, 'w_1315': 0, 'w_1316': 0, 'w_1317': 0, 'w_1318': 0, 'w_1319': 0, 'w_1320': 0, 'w_1321': 0, 'w_1322': 0, 'w_1323': 0, 'w_1324': 0, 'w_1325': 0, 'w_1326': 0, 'w_1327': 0, 'w_1328': 1, 'w_1329': 0, 'w_1330': 0, 'w_1331': 0, 'w_1332': 0, 'w_1333': 0, 'w_1334': 0, 'w_1335': 0, 'w_1336': 0, 'w_1337': 0, 'w_1338': 0, 'w_1339': 0, 'w_1340': 0, 'w_1341': 0, 'w_1342': 0, 'w_1343': 0, 'w_1344': 0, 'w_1345': 0, 'w_1346': 0, 'w_1347': 1, 'w_1348': 0, 'w_1349': 0, 'w_1350': 0, 'w_1351': 0, 'w_1352': 0, 'w_1353': 0, 'w_1354': 0, 'w_1355': 0, 'w_1356': 0, 'w_1357': 0, 'w_1358': 0, 'w_1359': 0, 'w_1360': 0, 'w_1361': 0, 'w_1362': 0, 'w_1363': 0, 'w_1364': 0, 'w_1365': 0, 'w_1366': 0, 'w_1367': 0, 'w_1368': 0, 'w_1369': 0, 'w_1370': 0, 'w_1371': 0, 'w_1372': 0, 'w_1373': 0, 'w_1374': 0, 'w_1375': 0, 'w_1376': 0, 'w_1377': 0, 'w_1378': 0, 'w_1379': 0, 'w_1380': 0, 'w_1381': 0, 'w_1382': 0, 'w_1383': 0, 'w_1384': 0, 'w_1385': 0, 'w_1386': 0, 'w_1387': 0, 'w_1388': 0, 'w_1389': 0, 'w_1390': 0, 'w_1391': 0, 'w_1392': 0, 'w_1393': 0, 'w_1394': 0, 'w_1395': 0, 'w_1396': 0, 'w_1397': 0, 'w_1398': 0, 'w_1399': 0, 'w_1400': 0, 'w_1401': 0, 'w_1402': 0, 'w_1403': 0, 'w_1404': 0, 'w_1405': 0, 'w_1406': 0, 'w_1407': 0, 'w_1408': 0, 'w_1409': 0, 'w_1410': 0, 'w_1411': 0, 'w_1412': 0, 'w_1413': 0, 'w_1414': 0, 'w_1415': 0, 'w_1416': 0, 'w_1417': 0, 'w_1418': 0, 'w_1419': 0, 'w_1420': 0, 'w_1421': 0, 'w_1422': 0, 'w_1423': 0, 'w_1424': 0, 'w_1425': 0, 'w_1426': 0, 'w_1427': 0, 'w_1428': 0, 'w_1429': 0, 'w_1430': 0, 'w_1431': 0, 'w_1432': 0, 'subject': 'Genetic_Algorithms'})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(G, type(G))\n",
    "\n",
    "print(list(G.nodes(data=True))[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('1033', '35', {})\n"
     ]
    }
   ],
   "source": [
    "edge = list(G.edges(data=True))[0]\n",
    "\n",
    "print(edge)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Convert the node attributes into integers"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['w_0', 'w_1', 'w_2', 'w_3', 'w_4', 'w_5', 'w_6', 'w_7', 'w_8', 'w_9', 'w_10', 'w_11', 'w_12', 'w_13', 'w_14', 'w_15', 'w_16', 'w_17', 'w_18', 'w_19', 'w_20', 'w_21', 'w_22', 'w_23', 'w_24', 'w_25', 'w_26', 'w_27', 'w_28', 'w_29', 'w_30', 'w_31', 'w_32', 'w_33', 'w_34', 'w_35', 'w_36', 'w_37', 'w_38', 'w_39', 'w_40', 'w_41', 'w_42', 'w_43', 'w_44', 'w_45', 'w_46', 'w_47', 'w_48', 'w_49', 'w_50', 'w_51', 'w_52', 'w_53', 'w_54', 'w_55', 'w_56', 'w_57', 'w_58', 'w_59', 'w_60', 'w_61', 'w_62', 'w_63', 'w_64', 'w_65', 'w_66', 'w_67', 'w_68', 'w_69', 'w_70', 'w_71', 'w_72', 'w_73', 'w_74', 'w_75', 'w_76', 'w_77', 'w_78', 'w_79', 'w_80', 'w_81', 'w_82', 'w_83', 'w_84', 'w_85', 'w_86', 'w_87', 'w_88', 'w_89', 'w_90', 'w_91', 'w_92', 'w_93', 'w_94', 'w_95', 'w_96', 'w_97', 'w_98', 'w_99', 'w_100', 'w_101', 'w_102', 'w_103', 'w_104', 'w_105', 'w_106', 'w_107', 'w_108', 'w_109', 'w_110', 'w_111', 'w_112', 'w_113', 'w_114', 'w_115', 'w_116', 'w_117', 'w_118', 'w_119', 'w_120', 'w_121', 'w_122', 'w_123', 'w_124', 'w_125', 'w_126', 'w_127', 'w_128', 'w_129', 'w_130', 'w_131', 'w_132', 'w_133', 'w_134', 'w_135', 'w_136', 'w_137', 'w_138', 'w_139', 'w_140', 'w_141', 'w_142', 'w_143', 'w_144', 'w_145', 'w_146', 'w_147', 'w_148', 'w_149', 'w_150', 'w_151', 'w_152', 'w_153', 'w_154', 'w_155', 'w_156', 'w_157', 'w_158', 'w_159', 'w_160', 'w_161', 'w_162', 'w_163', 'w_164', 'w_165', 'w_166', 'w_167', 'w_168', 'w_169', 'w_170', 'w_171', 'w_172', 'w_173', 'w_174', 'w_175', 'w_176', 'w_177', 'w_178', 'w_179', 'w_180', 'w_181', 'w_182', 'w_183', 'w_184', 'w_185', 'w_186', 'w_187', 'w_188', 'w_189', 'w_190', 'w_191', 'w_192', 'w_193', 'w_194', 'w_195', 'w_196', 'w_197', 'w_198', 'w_199', 'w_200', 'w_201', 'w_202', 'w_203', 'w_204', 'w_205', 'w_206', 'w_207', 'w_208', 'w_209', 'w_210', 'w_211', 'w_212', 'w_213', 'w_214', 'w_215', 'w_216', 'w_217', 'w_218', 'w_219', 'w_220', 'w_221', 'w_222', 'w_223', 'w_224', 'w_225', 'w_226', 'w_227', 'w_228', 'w_229', 'w_230', 'w_231', 'w_232', 'w_233', 'w_234', 'w_235', 'w_236', 'w_237', 'w_238', 'w_239', 'w_240', 'w_241', 'w_242', 'w_243', 'w_244', 'w_245', 'w_246', 'w_247', 'w_248', 'w_249', 'w_250', 'w_251', 'w_252', 'w_253', 'w_254', 'w_255', 'w_256', 'w_257', 'w_258', 'w_259', 'w_260', 'w_261', 'w_262', 'w_263', 'w_264', 'w_265', 'w_266', 'w_267', 'w_268', 'w_269', 'w_270', 'w_271', 'w_272', 'w_273', 'w_274', 'w_275', 'w_276', 'w_277', 'w_278', 'w_279', 'w_280', 'w_281', 'w_282', 'w_283', 'w_284', 'w_285', 'w_286', 'w_287', 'w_288', 'w_289', 'w_290', 'w_291', 'w_292', 'w_293', 'w_294', 'w_295', 'w_296', 'w_297', 'w_298', 'w_299', 'w_300', 'w_301', 'w_302', 'w_303', 'w_304', 'w_305', 'w_306', 'w_307', 'w_308', 'w_309', 'w_310', 'w_311', 'w_312', 'w_313', 'w_314', 'w_315', 'w_316', 'w_317', 'w_318', 'w_319', 'w_320', 'w_321', 'w_322', 'w_323', 'w_324', 'w_325', 'w_326', 'w_327', 'w_328', 'w_329', 'w_330', 'w_331', 'w_332', 'w_333', 'w_334', 'w_335', 'w_336', 'w_337', 'w_338', 'w_339', 'w_340', 'w_341', 'w_342', 'w_343', 'w_344', 'w_345', 'w_346', 'w_347', 'w_348', 'w_349', 'w_350', 'w_351', 'w_352', 'w_353', 'w_354', 'w_355', 'w_356', 'w_357', 'w_358', 'w_359', 'w_360', 'w_361', 'w_362', 'w_363', 'w_364', 'w_365', 'w_366', 'w_367', 'w_368', 'w_369', 'w_370', 'w_371', 'w_372', 'w_373', 'w_374', 'w_375', 'w_376', 'w_377', 'w_378', 'w_379', 'w_380', 'w_381', 'w_382', 'w_383', 'w_384', 'w_385', 'w_386', 'w_387', 'w_388', 'w_389', 'w_390', 'w_391', 'w_392', 'w_393', 'w_394', 'w_395', 'w_396', 'w_397', 'w_398', 'w_399', 'w_400', 'w_401', 'w_402', 'w_403', 'w_404', 'w_405', 'w_406', 'w_407', 'w_408', 'w_409', 'w_410', 'w_411', 'w_412', 'w_413', 'w_414', 'w_415', 'w_416', 'w_417', 'w_418', 'w_419', 'w_420', 'w_421', 'w_422', 'w_423', 'w_424', 'w_425', 'w_426', 'w_427', 'w_428', 'w_429', 'w_430', 'w_431', 'w_432', 'w_433', 'w_434', 'w_435', 'w_436', 'w_437', 'w_438', 'w_439', 'w_440', 'w_441', 'w_442', 'w_443', 'w_444', 'w_445', 'w_446', 'w_447', 'w_448', 'w_449', 'w_450', 'w_451', 'w_452', 'w_453', 'w_454', 'w_455', 'w_456', 'w_457', 'w_458', 'w_459', 'w_460', 'w_461', 'w_462', 'w_463', 'w_464', 'w_465', 'w_466', 'w_467', 'w_468', 'w_469', 'w_470', 'w_471', 'w_472', 'w_473', 'w_474', 'w_475', 'w_476', 'w_477', 'w_478', 'w_479', 'w_480', 'w_481', 'w_482', 'w_483', 'w_484', 'w_485', 'w_486', 'w_487', 'w_488', 'w_489', 'w_490', 'w_491', 'w_492', 'w_493', 'w_494', 'w_495', 'w_496', 'w_497', 'w_498', 'w_499', 'w_500', 'w_501', 'w_502', 'w_503', 'w_504', 'w_505', 'w_506', 'w_507', 'w_508', 'w_509', 'w_510', 'w_511', 'w_512', 'w_513', 'w_514', 'w_515', 'w_516', 'w_517', 'w_518', 'w_519', 'w_520', 'w_521', 'w_522', 'w_523', 'w_524', 'w_525', 'w_526', 'w_527', 'w_528', 'w_529', 'w_530', 'w_531', 'w_532', 'w_533', 'w_534', 'w_535', 'w_536', 'w_537', 'w_538', 'w_539', 'w_540', 'w_541', 'w_542', 'w_543', 'w_544', 'w_545', 'w_546', 'w_547', 'w_548', 'w_549', 'w_550', 'w_551', 'w_552', 'w_553', 'w_554', 'w_555', 'w_556', 'w_557', 'w_558', 'w_559', 'w_560', 'w_561', 'w_562', 'w_563', 'w_564', 'w_565', 'w_566', 'w_567', 'w_568', 'w_569', 'w_570', 'w_571', 'w_572', 'w_573', 'w_574', 'w_575', 'w_576', 'w_577', 'w_578', 'w_579', 'w_580', 'w_581', 'w_582', 'w_583', 'w_584', 'w_585', 'w_586', 'w_587', 'w_588', 'w_589', 'w_590', 'w_591', 'w_592', 'w_593', 'w_594', 'w_595', 'w_596', 'w_597', 'w_598', 'w_599', 'w_600', 'w_601', 'w_602', 'w_603', 'w_604', 'w_605', 'w_606', 'w_607', 'w_608', 'w_609', 'w_610', 'w_611', 'w_612', 'w_613', 'w_614', 'w_615', 'w_616', 'w_617', 'w_618', 'w_619', 'w_620', 'w_621', 'w_622', 'w_623', 'w_624', 'w_625', 'w_626', 'w_627', 'w_628', 'w_629', 'w_630', 'w_631', 'w_632', 'w_633', 'w_634', 'w_635', 'w_636', 'w_637', 'w_638', 'w_639', 'w_640', 'w_641', 'w_642', 'w_643', 'w_644', 'w_645', 'w_646', 'w_647', 'w_648', 'w_649', 'w_650', 'w_651', 'w_652', 'w_653', 'w_654', 'w_655', 'w_656', 'w_657', 'w_658', 'w_659', 'w_660', 'w_661', 'w_662', 'w_663', 'w_664', 'w_665', 'w_666', 'w_667', 'w_668', 'w_669', 'w_670', 'w_671', 'w_672', 'w_673', 'w_674', 'w_675', 'w_676', 'w_677', 'w_678', 'w_679', 'w_680', 'w_681', 'w_682', 'w_683', 'w_684', 'w_685', 'w_686', 'w_687', 'w_688', 'w_689', 'w_690', 'w_691', 'w_692', 'w_693', 'w_694', 'w_695', 'w_696', 'w_697', 'w_698', 'w_699', 'w_700', 'w_701', 'w_702', 'w_703', 'w_704', 'w_705', 'w_706', 'w_707', 'w_708', 'w_709', 'w_710', 'w_711', 'w_712', 'w_713', 'w_714', 'w_715', 'w_716', 'w_717', 'w_718', 'w_719', 'w_720', 'w_721', 'w_722', 'w_723', 'w_724', 'w_725', 'w_726', 'w_727', 'w_728', 'w_729', 'w_730', 'w_731', 'w_732', 'w_733', 'w_734', 'w_735', 'w_736', 'w_737', 'w_738', 'w_739', 'w_740', 'w_741', 'w_742', 'w_743', 'w_744', 'w_745', 'w_746', 'w_747', 'w_748', 'w_749', 'w_750', 'w_751', 'w_752', 'w_753', 'w_754', 'w_755', 'w_756', 'w_757', 'w_758', 'w_759', 'w_760', 'w_761', 'w_762', 'w_763', 'w_764', 'w_765', 'w_766', 'w_767', 'w_768', 'w_769', 'w_770', 'w_771', 'w_772', 'w_773', 'w_774', 'w_775', 'w_776', 'w_777', 'w_778', 'w_779', 'w_780', 'w_781', 'w_782', 'w_783', 'w_784', 'w_785', 'w_786', 'w_787', 'w_788', 'w_789', 'w_790', 'w_791', 'w_792', 'w_793', 'w_794', 'w_795', 'w_796', 'w_797', 'w_798', 'w_799', 'w_800', 'w_801', 'w_802', 'w_803', 'w_804', 'w_805', 'w_806', 'w_807', 'w_808', 'w_809', 'w_810', 'w_811', 'w_812', 'w_813', 'w_814', 'w_815', 'w_816', 'w_817', 'w_818', 'w_819', 'w_820', 'w_821', 'w_822', 'w_823', 'w_824', 'w_825', 'w_826', 'w_827', 'w_828', 'w_829', 'w_830', 'w_831', 'w_832', 'w_833', 'w_834', 'w_835', 'w_836', 'w_837', 'w_838', 'w_839', 'w_840', 'w_841', 'w_842', 'w_843', 'w_844', 'w_845', 'w_846', 'w_847', 'w_848', 'w_849', 'w_850', 'w_851', 'w_852', 'w_853', 'w_854', 'w_855', 'w_856', 'w_857', 'w_858', 'w_859', 'w_860', 'w_861', 'w_862', 'w_863', 'w_864', 'w_865', 'w_866', 'w_867', 'w_868', 'w_869', 'w_870', 'w_871', 'w_872', 'w_873', 'w_874', 'w_875', 'w_876', 'w_877', 'w_878', 'w_879', 'w_880', 'w_881', 'w_882', 'w_883', 'w_884', 'w_885', 'w_886', 'w_887', 'w_888', 'w_889', 'w_890', 'w_891', 'w_892', 'w_893', 'w_894', 'w_895', 'w_896', 'w_897', 'w_898', 'w_899', 'w_900', 'w_901', 'w_902', 'w_903', 'w_904', 'w_905', 'w_906', 'w_907', 'w_908', 'w_909', 'w_910', 'w_911', 'w_912', 'w_913', 'w_914', 'w_915', 'w_916', 'w_917', 'w_918', 'w_919', 'w_920', 'w_921', 'w_922', 'w_923', 'w_924', 'w_925', 'w_926', 'w_927', 'w_928', 'w_929', 'w_930', 'w_931', 'w_932', 'w_933', 'w_934', 'w_935', 'w_936', 'w_937', 'w_938', 'w_939', 'w_940', 'w_941', 'w_942', 'w_943', 'w_944', 'w_945', 'w_946', 'w_947', 'w_948', 'w_949', 'w_950', 'w_951', 'w_952', 'w_953', 'w_954', 'w_955', 'w_956', 'w_957', 'w_958', 'w_959', 'w_960', 'w_961', 'w_962', 'w_963', 'w_964', 'w_965', 'w_966', 'w_967', 'w_968', 'w_969', 'w_970', 'w_971', 'w_972', 'w_973', 'w_974', 'w_975', 'w_976', 'w_977', 'w_978', 'w_979', 'w_980', 'w_981', 'w_982', 'w_983', 'w_984', 'w_985', 'w_986', 'w_987', 'w_988', 'w_989', 'w_990', 'w_991', 'w_992', 'w_993', 'w_994', 'w_995', 'w_996', 'w_997', 'w_998', 'w_999', 'w_1000', 'w_1001', 'w_1002', 'w_1003', 'w_1004', 'w_1005', 'w_1006', 'w_1007', 'w_1008', 'w_1009', 'w_1010', 'w_1011', 'w_1012', 'w_1013', 'w_1014', 'w_1015', 'w_1016', 'w_1017', 'w_1018', 'w_1019', 'w_1020', 'w_1021', 'w_1022', 'w_1023', 'w_1024', 'w_1025', 'w_1026', 'w_1027', 'w_1028', 'w_1029', 'w_1030', 'w_1031', 'w_1032', 'w_1033', 'w_1034', 'w_1035', 'w_1036', 'w_1037', 'w_1038', 'w_1039', 'w_1040', 'w_1041', 'w_1042', 'w_1043', 'w_1044', 'w_1045', 'w_1046', 'w_1047', 'w_1048', 'w_1049', 'w_1050', 'w_1051', 'w_1052', 'w_1053', 'w_1054', 'w_1055', 'w_1056', 'w_1057', 'w_1058', 'w_1059', 'w_1060', 'w_1061', 'w_1062', 'w_1063', 'w_1064', 'w_1065', 'w_1066', 'w_1067', 'w_1068', 'w_1069', 'w_1070', 'w_1071', 'w_1072', 'w_1073', 'w_1074', 'w_1075', 'w_1076', 'w_1077', 'w_1078', 'w_1079', 'w_1080', 'w_1081', 'w_1082', 'w_1083', 'w_1084', 'w_1085', 'w_1086', 'w_1087', 'w_1088', 'w_1089', 'w_1090', 'w_1091', 'w_1092', 'w_1093', 'w_1094', 'w_1095', 'w_1096', 'w_1097', 'w_1098', 'w_1099', 'w_1100', 'w_1101', 'w_1102', 'w_1103', 'w_1104', 'w_1105', 'w_1106', 'w_1107', 'w_1108', 'w_1109', 'w_1110', 'w_1111', 'w_1112', 'w_1113', 'w_1114', 'w_1115', 'w_1116', 'w_1117', 'w_1118', 'w_1119', 'w_1120', 'w_1121', 'w_1122', 'w_1123', 'w_1124', 'w_1125', 'w_1126', 'w_1127', 'w_1128', 'w_1129', 'w_1130', 'w_1131', 'w_1132', 'w_1133', 'w_1134', 'w_1135', 'w_1136', 'w_1137', 'w_1138', 'w_1139', 'w_1140', 'w_1141', 'w_1142', 'w_1143', 'w_1144', 'w_1145', 'w_1146', 'w_1147', 'w_1148', 'w_1149', 'w_1150', 'w_1151', 'w_1152', 'w_1153', 'w_1154', 'w_1155', 'w_1156', 'w_1157', 'w_1158', 'w_1159', 'w_1160', 'w_1161', 'w_1162', 'w_1163', 'w_1164', 'w_1165', 'w_1166', 'w_1167', 'w_1168', 'w_1169', 'w_1170', 'w_1171', 'w_1172', 'w_1173', 'w_1174', 'w_1175', 'w_1176', 'w_1177', 'w_1178', 'w_1179', 'w_1180', 'w_1181', 'w_1182', 'w_1183', 'w_1184', 'w_1185', 'w_1186', 'w_1187', 'w_1188', 'w_1189', 'w_1190', 'w_1191', 'w_1192', 'w_1193', 'w_1194', 'w_1195', 'w_1196', 'w_1197', 'w_1198', 'w_1199', 'w_1200', 'w_1201', 'w_1202', 'w_1203', 'w_1204', 'w_1205', 'w_1206', 'w_1207', 'w_1208', 'w_1209', 'w_1210', 'w_1211', 'w_1212', 'w_1213', 'w_1214', 'w_1215', 'w_1216', 'w_1217', 'w_1218', 'w_1219', 'w_1220', 'w_1221', 'w_1222', 'w_1223', 'w_1224', 'w_1225', 'w_1226', 'w_1227', 'w_1228', 'w_1229', 'w_1230', 'w_1231', 'w_1232', 'w_1233', 'w_1234', 'w_1235', 'w_1236', 'w_1237', 'w_1238', 'w_1239', 'w_1240', 'w_1241', 'w_1242', 'w_1243', 'w_1244', 'w_1245', 'w_1246', 'w_1247', 'w_1248', 'w_1249', 'w_1250', 'w_1251', 'w_1252', 'w_1253', 'w_1254', 'w_1255', 'w_1256', 'w_1257', 'w_1258', 'w_1259', 'w_1260', 'w_1261', 'w_1262', 'w_1263', 'w_1264', 'w_1265', 'w_1266', 'w_1267', 'w_1268', 'w_1269', 'w_1270', 'w_1271', 'w_1272', 'w_1273', 'w_1274', 'w_1275', 'w_1276', 'w_1277', 'w_1278', 'w_1279', 'w_1280', 'w_1281', 'w_1282', 'w_1283', 'w_1284', 'w_1285', 'w_1286', 'w_1287', 'w_1288', 'w_1289', 'w_1290', 'w_1291', 'w_1292', 'w_1293', 'w_1294', 'w_1295', 'w_1296', 'w_1297', 'w_1298', 'w_1299', 'w_1300', 'w_1301', 'w_1302', 'w_1303', 'w_1304', 'w_1305', 'w_1306', 'w_1307', 'w_1308', 'w_1309', 'w_1310', 'w_1311', 'w_1312', 'w_1313', 'w_1314', 'w_1315', 'w_1316', 'w_1317', 'w_1318', 'w_1319', 'w_1320', 'w_1321', 'w_1322', 'w_1323', 'w_1324', 'w_1325', 'w_1326', 'w_1327', 'w_1328', 'w_1329', 'w_1330', 'w_1331', 'w_1332', 'w_1333', 'w_1334', 'w_1335', 'w_1336', 'w_1337', 'w_1338', 'w_1339', 'w_1340', 'w_1341', 'w_1342', 'w_1343', 'w_1344', 'w_1345', 'w_1346', 'w_1347', 'w_1348', 'w_1349', 'w_1350', 'w_1351', 'w_1352', 'w_1353', 'w_1354', 'w_1355', 'w_1356', 'w_1357', 'w_1358', 'w_1359', 'w_1360', 'w_1361', 'w_1362', 'w_1363', 'w_1364', 'w_1365', 'w_1366', 'w_1367', 'w_1368', 'w_1369', 'w_1370', 'w_1371', 'w_1372', 'w_1373', 'w_1374', 'w_1375', 'w_1376', 'w_1377', 'w_1378', 'w_1379', 'w_1380', 'w_1381', 'w_1382', 'w_1383', 'w_1384', 'w_1385', 'w_1386', 'w_1387', 'w_1388', 'w_1389', 'w_1390', 'w_1391', 'w_1392', 'w_1393', 'w_1394', 'w_1395', 'w_1396', 'w_1397', 'w_1398', 'w_1399', 'w_1400', 'w_1401', 'w_1402', 'w_1403', 'w_1404', 'w_1405', 'w_1406', 'w_1407', 'w_1408', 'w_1409', 'w_1410', 'w_1411', 'w_1412', 'w_1413', 'w_1414', 'w_1415', 'w_1416', 'w_1417', 'w_1418', 'w_1419', 'w_1420', 'w_1421', 'w_1422', 'w_1423', 'w_1424', 'w_1425', 'w_1426', 'w_1427', 'w_1428', 'w_1429', 'w_1430', 'w_1431', 'w_1432', 'subject']\n",
      "{0, 1, 2, 3, 4, 5, 6}\n",
      "1434\n"
     ]
    }
   ],
   "source": [
    "classes = set()\n",
    "for id in G.nodes:\n",
    "    classes.add(G.nodes[id]['subject'])\n",
    "\n",
    "labels = {'Probabilistic_Methods':0,\n",
    "          'Theory':1,\n",
    "          'Reinforcement_Learning':2,\n",
    "          'Case_Based':3,\n",
    "          'Genetic_Algorithms':4,\n",
    "          'Neural_Networks':5,\n",
    "          'Rule_Learning':6,\n",
    "          0:0,1:1,2:2,3:3,4:4,5:5,6:6}\n",
    "for id in G.nodes:\n",
    "    G.nodes[id]['subject'] = labels[G.nodes[id]['subject']]\n",
    "\n",
    "g_node_attrs = list(list(G.nodes(data=True))[0][1].keys())\n",
    "\n",
    "print(g_node_attrs)\n",
    "\n",
    "\n",
    "\n",
    "num_classes = len(classes)\n",
    "\n",
    "print(classes)\n",
    "\n",
    "w_attrs = [attr for attr in g_node_attrs if 'w_' in attr]\n",
    "\n",
    "print(len(g_node_attrs))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{} <class 'dict'>\n",
      "{} <class 'dict'>\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# delete all the graph attributes, since they are not useful for node classification\n",
    "\n",
    "# print(type(node_attrs))\n",
    "# print(edge_attrs)\n",
    "\n",
    "# graph.graph['node_default'] = True\n",
    "# graph.graph['edge_default'] = True\n",
    "\n",
    "print(G.graph, type(G.graph))\n",
    "\n",
    "if 'node_default' in G.graph:\n",
    "    del G.graph['node_default']\n",
    "if 'edge_default' in G.graph:\n",
    "    del G.graph['edge_default']\n",
    "\n",
    "if 'edge_index' in G.graph:\n",
    "    del G.graph['edge_index']\n",
    "\n",
    "attrs = [attr for attr, _ in G.graph.items()]\n",
    "\n",
    "for attr in attrs:\n",
    "    del G.graph[attr]\n",
    "\n",
    "print(G.graph, type(G.graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of edges: 10556\n",
      "Data(edge_index=[2, 10556], x=[2708, 1434]) <class 'torch_geometric.data.data.Data'>\n"
     ]
    }
   ],
   "source": [
    "data = conv.from_networkx(G, group_node_attrs=g_node_attrs)\n",
    "\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "\n",
    "print(data, type(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Extract the classification label from the nodes attributes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0,  ..., 0, 0, 4],\n",
      "        [0, 0, 0,  ..., 0, 0, 4],\n",
      "        [0, 0, 0,  ..., 0, 0, 5],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 5],\n",
      "        [0, 0, 0,  ..., 0, 0, 5],\n",
      "        [0, 0, 0,  ..., 0, 0, 5]])\n",
      "torch.Size([2708, 1434])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([5, 0, 1,  ..., 1, 5, 3])\n",
      "Data(edge_index=[2, 10556], x=[2708, 1433], y=[2708])\n"
     ]
    }
   ],
   "source": [
    "print(data.x)\n",
    "\n",
    "# shuffle the data by row\n",
    "data.x = data.x[torch.randperm(data.x.size()[0])]\n",
    "print(data.x.shape)\n",
    "\n",
    "# get the classification attibute from x and assign it to y\n",
    "data.y = data.x[:, -1]\n",
    "# data.y = torch.stack([y[0] for y in data.y])\n",
    "\n",
    "# remove the classification attribute from x\n",
    "data.x = data.x[:, :-1].float()\n",
    "\n",
    "print(data.x)\n",
    "print(data.y)\n",
    "\n",
    "print(data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define training, validation and test sets randomly"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(edge_index=[2, 10556], x=[2708, 1433], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.transforms import RandomNodeSplit\n",
    "\n",
    "# split the data in train, validation and test sets\n",
    "# it adds to data train, val and test masks\n",
    "rns = RandomNodeSplit('random', num_val=0.1, num_test=0.2)\n",
    "\n",
    "data = rns(data)\n",
    "\n",
    "print(data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "One-hot encode the classification labels"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "Data(edge_index=[2, 10556], x=[2708, 1433], y=[2708, 7], train_mask=[2708], val_mask=[2708], test_mask=[2708])\n"
     ]
    }
   ],
   "source": [
    "# one hot encode labels\n",
    "data.y = torch.nn.functional.one_hot(data.y.long(), num_classes=num_classes).to(torch.float)\n",
    "\n",
    "print(data.y)\n",
    "print(data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "torch.float32\n",
      "torch.int64\n"
     ]
    }
   ],
   "source": [
    "print(data.x.dtype)\n",
    "print(data.y.dtype)\n",
    "print(data.edge_index.dtype)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([   0,    0,    0,  ..., 2706, 2707, 2707]), tensor([   1,  324,  330,  ..., 2705,  729, 2705])]\n"
     ]
    }
   ],
   "source": [
    "print(list(data.edge_index))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def visualize(h, color):\n",
    "    z = TSNE(n_components=2).fit_transform(h.detach().cpu().numpy())\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "    plt.scatter(z[:, 0], z[:, 1], s=70, c=color, cmap=\"Set2\")\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (conv1): GCNConv(1433, 16)\n",
      "  (conv2): GCNConv(16, 7)\n",
      ")\n",
      "tensor([[0.1439, 0.1411, 0.1427,  ..., 0.1414, 0.1482, 0.1417],\n",
      "        [0.1649, 0.1386, 0.1328,  ..., 0.1318, 0.1688, 0.1315],\n",
      "        [0.1472, 0.1418, 0.1401,  ..., 0.1413, 0.1499, 0.1408],\n",
      "        ...,\n",
      "        [0.1505, 0.1378, 0.1378,  ..., 0.1405, 0.1562, 0.1378],\n",
      "        [0.1500, 0.1395, 0.1395,  ..., 0.1395, 0.1514, 0.1395],\n",
      "        [0.1517, 0.1400, 0.1354,  ..., 0.1426, 0.1559, 0.1354]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, input_features, hidden_channels):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(1234567)\n",
    "        self.conv1 = GCNConv(input_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        # x = softmax(x)\n",
    "        # x = F.log_softmax(x, dim=1)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x\n",
    "\n",
    "in_features = data.x.shape[1]\n",
    "\n",
    "model = GCN(input_features=in_features, hidden_channels=16)\n",
    "model.eval()\n",
    "print(model)\n",
    "\n",
    "out = model(data.x, data.edge_index)\n",
    "#visualize(out, color=data.y)\n",
    "print(out)\n",
    "\n",
    "model = GCN(input_features=in_features, hidden_channels=64)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1, weight_decay=5e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "outputs": [],
   "source": [
    "class SAGE(torch.nn.Module):\n",
    "    def __init__(self, input_features, hidden_channels):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(1234567)\n",
    "        self.conv1 = SAGEConv(input_features, hidden_channels)\n",
    "        self.conv2 = SAGEConv(hidden_channels, num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x\n",
    "\n",
    "in_features = data.x.shape[1]\n",
    "\n",
    "model = SAGE(input_features=in_features, hidden_channels=64)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1, weight_decay=5e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAT(\n",
      "  (conv1): GATConv(1433, 8, heads=8)\n",
      "  (conv2): GATConv(64, 7, heads=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, heads):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(1234567)\n",
    "\n",
    "        self.hid = 8\n",
    "        self.in_head = 8\n",
    "        self.out_head = 1\n",
    "\n",
    "        self.conv1 = GATConv(in_features, self.hid, heads=self.in_head, dropout=0.6)\n",
    "        self.conv2 = GATConv(self.hid*self.in_head, num_classes, concat=False,\n",
    "                             heads=self.out_head, dropout=0.6)\n",
    "\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        return F.softmax(x, dim=1)\n",
    "\n",
    "model = GAT(hidden_channels=8, heads=8)\n",
    "print(model)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "outputs": [],
   "source": [
    "def train():\n",
    "      model.train()\n",
    "      optimizer.zero_grad()  # Clear gradients.\n",
    "      out = model(data.x, data.edge_index)  # Perform a single forward pass.\n",
    "      loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "      val_loss = criterion(out[data.val_mask], data.y[data.val_mask])  # Compute the loss solely based on the training nodes.\n",
    "      loss.backward()  # Derive gradients.\n",
    "      optimizer.step()  # Update parameters based on gradients.\n",
    "      return loss, val_loss\n",
    "\n",
    "\n",
    "def test(models, ds='test'):\n",
    "      models.eval()\n",
    "      out = models(data.x, data.edge_index)\n",
    "      pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "      if ds == 'test':\n",
    "            outs = out[data.test_mask]\n",
    "            mask = data.test_mask\n",
    "            d_set = pred[data.test_mask]\n",
    "            label_set = data.y[data.test_mask]\n",
    "      elif ds == 'val':\n",
    "            outs = out[data.val_mask]\n",
    "            mask = data.val_mask\n",
    "            d_set = pred[data.val_mask]\n",
    "            label_set = data.y[data.val_mask]\n",
    "      else:\n",
    "            outs = out[data.train_mask]\n",
    "            mask = data.train_mask\n",
    "            d_set = pred[data.train_mask]\n",
    "            label_set = data.y[data.train_mask]\n",
    "      print(outs)\n",
    "      print(label_set)\n",
    "      label_set = label_set.argmax(dim=1)\n",
    "      test_correct = d_set == label_set  # Check against ground-truth labels.\n",
    "      test_acc = int(test_correct.sum()) / int(mask.sum())  # Derive ratio of correct predictions.\n",
    "      return test_acc"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 1.9460, Val Loss: 1.9456\n",
      "Epoch: 002, Loss: 1.9290, Val Loss: 1.9762\n",
      "Epoch: 003, Loss: 1.8945, Val Loss: 1.9893\n",
      "Epoch: 004, Loss: 1.8162, Val Loss: 1.9501\n",
      "Epoch: 005, Loss: 1.8269, Val Loss: 1.9155\n",
      "Epoch: 006, Loss: 1.7592, Val Loss: 1.9117\n",
      "Epoch: 007, Loss: 1.7146, Val Loss: 1.9636\n",
      "Epoch: 008, Loss: 1.6222, Val Loss: 1.9073\n",
      "Epoch: 009, Loss: 1.6570, Val Loss: 1.9672\n",
      "Epoch: 010, Loss: 1.5744, Val Loss: 1.9602\n",
      "Epoch: 011, Loss: 1.5380, Val Loss: 1.8775\n",
      "Epoch: 012, Loss: 1.5186, Val Loss: 1.8674\n",
      "Epoch: 013, Loss: 1.4628, Val Loss: 1.9254\n",
      "Epoch: 014, Loss: 1.4287, Val Loss: 1.9179\n",
      "Epoch: 015, Loss: 1.4827, Val Loss: 1.9004\n",
      "Epoch: 016, Loss: 1.4091, Val Loss: 1.8846\n",
      "Epoch: 017, Loss: 1.4969, Val Loss: 1.8544\n",
      "Epoch: 018, Loss: 1.4423, Val Loss: 1.8878\n",
      "Epoch: 019, Loss: 1.4261, Val Loss: 1.8937\n",
      "Epoch: 020, Loss: 1.4522, Val Loss: 1.8653\n",
      "Epoch: 021, Loss: 1.3979, Val Loss: 1.8792\n",
      "Epoch: 022, Loss: 1.3536, Val Loss: 1.8772\n",
      "Epoch: 023, Loss: 1.3994, Val Loss: 1.9012\n",
      "Epoch: 024, Loss: 1.3723, Val Loss: 1.8770\n",
      "Epoch: 025, Loss: 1.4065, Val Loss: 1.8856\n",
      "Epoch: 026, Loss: 1.3306, Val Loss: 1.8795\n",
      "Epoch: 027, Loss: 1.3385, Val Loss: 1.8787\n",
      "tensor([[2.1815e-02, 2.1815e-02, 2.1815e-02, 2.4553e-02, 2.1815e-02, 8.6637e-01,\n",
      "         2.1815e-02],\n",
      "        [3.6049e-02, 3.6049e-02, 7.0096e-02, 3.6049e-02, 3.6049e-02, 3.6049e-02,\n",
      "         7.4966e-01],\n",
      "        [1.0113e-03, 7.7653e-04, 7.7653e-04, 7.7653e-04, 1.1240e-03, 9.9476e-01,\n",
      "         7.7653e-04],\n",
      "        [5.0388e-02, 6.9076e-01, 5.0388e-02, 5.0388e-02, 5.0388e-02, 5.3948e-02,\n",
      "         5.3740e-02],\n",
      "        [1.8256e-03, 2.1132e-03, 1.8256e-03, 1.8256e-03, 9.8560e-01, 4.9872e-03,\n",
      "         1.8256e-03],\n",
      "        [1.1256e-03, 1.1256e-03, 1.1256e-03, 1.1256e-03, 1.1256e-03, 4.1137e-03,\n",
      "         9.9026e-01],\n",
      "        [1.2056e-02, 1.2056e-02, 1.2056e-02, 9.2261e-01, 1.2056e-02, 1.2056e-02,\n",
      "         1.7113e-02],\n",
      "        [2.3832e-02, 2.3832e-02, 2.3832e-02, 8.3734e-01, 2.3832e-02, 2.3832e-02,\n",
      "         4.3497e-02],\n",
      "        [1.2476e-02, 1.2476e-02, 1.2476e-02, 9.1286e-01, 1.2476e-02, 2.4759e-02,\n",
      "         1.2476e-02],\n",
      "        [9.5151e-01, 8.1170e-03, 6.1415e-03, 1.5805e-02, 6.1415e-03, 6.1415e-03,\n",
      "         6.1415e-03],\n",
      "        [5.2854e-03, 6.6334e-03, 5.2854e-03, 5.2854e-03, 9.6679e-01, 5.4365e-03,\n",
      "         5.2854e-03],\n",
      "        [1.7402e-03, 1.7402e-03, 1.7402e-03, 1.9159e-03, 1.7402e-03, 1.6910e-02,\n",
      "         9.7421e-01],\n",
      "        [1.4314e-02, 1.4314e-02, 1.4314e-02, 1.4314e-02, 8.9227e-01, 1.4314e-02,\n",
      "         3.6167e-02],\n",
      "        [3.3088e-04, 2.0825e-04, 2.0825e-04, 2.0825e-04, 2.0825e-04, 9.9863e-01,\n",
      "         2.0825e-04],\n",
      "        [8.5822e-01, 2.1434e-02, 1.7572e-02, 2.1440e-02, 3.8443e-02, 2.5319e-02,\n",
      "         1.7572e-02],\n",
      "        [6.0655e-03, 9.6232e-01, 6.0655e-03, 6.0655e-03, 7.3529e-03, 6.0655e-03,\n",
      "         6.0655e-03],\n",
      "        [1.0365e-02, 1.0365e-02, 1.0976e-02, 9.3242e-01, 1.0365e-02, 1.2374e-02,\n",
      "         1.3134e-02],\n",
      "        [1.0403e-02, 1.0403e-02, 1.0403e-02, 8.9777e-01, 3.2669e-02, 2.7950e-02,\n",
      "         1.0403e-02],\n",
      "        [3.1613e-03, 4.0745e-03, 3.1613e-03, 3.1613e-03, 3.1613e-03, 3.1613e-03,\n",
      "         9.8012e-01],\n",
      "        [6.3098e-03, 6.3098e-03, 6.3098e-03, 1.1798e-02, 9.5137e-01, 6.3098e-03,\n",
      "         1.1596e-02],\n",
      "        [4.7847e-03, 4.7847e-03, 4.7847e-03, 9.6802e-01, 4.7847e-03, 4.7847e-03,\n",
      "         8.0557e-03],\n",
      "        [9.9489e-01, 7.9232e-04, 7.9232e-04, 7.9232e-04, 7.9232e-04, 1.1520e-03,\n",
      "         7.9232e-04],\n",
      "        [5.8303e-03, 5.8303e-03, 5.8303e-03, 6.4687e-03, 1.0515e-02, 9.5970e-01,\n",
      "         5.8303e-03],\n",
      "        [3.4711e-02, 6.2625e-02, 3.4711e-02, 3.4711e-02, 3.4711e-02, 3.4931e-02,\n",
      "         7.6360e-01],\n",
      "        [3.5152e-03, 3.5152e-03, 4.1678e-03, 3.5152e-03, 3.5152e-03, 9.7826e-01,\n",
      "         3.5152e-03],\n",
      "        [1.4754e-02, 2.2810e-02, 1.4754e-02, 3.3589e-02, 8.6865e-01, 1.4754e-02,\n",
      "         3.0691e-02],\n",
      "        [6.2821e-02, 5.8196e-01, 6.2821e-02, 6.2821e-02, 6.2821e-02, 6.2821e-02,\n",
      "         1.0394e-01],\n",
      "        [1.7515e-02, 8.8829e-01, 1.7515e-02, 1.7515e-02, 1.7515e-02, 1.7515e-02,\n",
      "         2.4135e-02],\n",
      "        [8.3612e-03, 1.4735e-02, 1.1550e-02, 1.0577e-02, 8.3612e-03, 8.3612e-03,\n",
      "         9.3805e-01],\n",
      "        [8.6821e-03, 8.6821e-03, 9.4178e-01, 8.6821e-03, 8.6821e-03, 8.6821e-03,\n",
      "         1.4814e-02],\n",
      "        [4.1030e-03, 4.1030e-03, 4.1030e-03, 4.1030e-03, 4.1030e-03, 2.1209e-02,\n",
      "         9.5828e-01],\n",
      "        [6.8504e-03, 6.8504e-03, 9.3613e-01, 6.9704e-03, 6.8504e-03, 6.8504e-03,\n",
      "         2.9494e-02],\n",
      "        [2.0200e-02, 2.1497e-02, 2.0200e-02, 3.3979e-02, 8.6372e-01, 2.0200e-02,\n",
      "         2.0200e-02],\n",
      "        [8.1827e-03, 8.1827e-03, 8.1827e-03, 8.1827e-03, 1.6062e-02, 9.4302e-01,\n",
      "         8.1827e-03],\n",
      "        [1.0110e-02, 1.0110e-02, 1.0110e-02, 1.0110e-02, 8.9911e-01, 5.0344e-02,\n",
      "         1.0110e-02],\n",
      "        [4.2381e-03, 4.2381e-03, 4.2381e-03, 4.2381e-03, 9.6273e-01, 1.6083e-02,\n",
      "         4.2381e-03],\n",
      "        [1.3088e-02, 9.0573e-01, 1.3088e-02, 1.3088e-02, 2.8836e-02, 1.3088e-02,\n",
      "         1.3088e-02],\n",
      "        [2.8000e-03, 2.8000e-03, 2.8000e-03, 5.3430e-03, 9.7785e-01, 5.6050e-03,\n",
      "         2.8000e-03],\n",
      "        [6.6922e-03, 1.4960e-02, 9.5158e-01, 6.6922e-03, 6.6922e-03, 6.6922e-03,\n",
      "         6.6922e-03],\n",
      "        [3.3237e-02, 3.3237e-02, 6.0636e-02, 7.5506e-01, 3.3237e-02, 3.3237e-02,\n",
      "         5.1358e-02],\n",
      "        [4.7655e-03, 9.7141e-01, 4.7655e-03, 4.7655e-03, 4.7655e-03, 4.7655e-03,\n",
      "         4.7655e-03],\n",
      "        [4.2990e-03, 4.2990e-03, 9.5487e-01, 1.9271e-02, 4.2990e-03, 4.2990e-03,\n",
      "         8.6680e-03],\n",
      "        [7.0183e-03, 7.0183e-03, 8.5891e-03, 7.0183e-03, 8.1403e-03, 9.5520e-01,\n",
      "         7.0183e-03],\n",
      "        [2.4755e-03, 2.4755e-03, 2.2013e-02, 2.4755e-03, 2.4755e-03, 9.6561e-01,\n",
      "         2.4755e-03],\n",
      "        [2.5168e-03, 2.5168e-03, 9.6553e-01, 2.5168e-03, 2.5168e-03, 1.8663e-02,\n",
      "         5.7424e-03],\n",
      "        [9.9945e-01, 7.5475e-05, 8.2211e-05, 1.4039e-04, 7.0791e-05, 1.1212e-04,\n",
      "         7.0791e-05],\n",
      "        [1.7434e-02, 1.9165e-02, 1.7434e-02, 1.8977e-02, 3.9147e-02, 8.7041e-01,\n",
      "         1.7434e-02],\n",
      "        [2.5267e-03, 9.8484e-01, 2.5267e-03, 2.5267e-03, 2.5267e-03, 2.5267e-03,\n",
      "         2.5267e-03],\n",
      "        [9.6498e-01, 2.7977e-03, 9.4810e-03, 1.4096e-02, 2.7977e-03, 3.0492e-03,\n",
      "         2.7977e-03],\n",
      "        [2.4740e-02, 1.7664e-02, 1.7664e-02, 8.7748e-01, 1.7664e-02, 2.2974e-02,\n",
      "         2.1815e-02],\n",
      "        [3.3223e-03, 3.3223e-03, 6.2717e-03, 9.7712e-01, 3.3223e-03, 3.3223e-03,\n",
      "         3.3223e-03],\n",
      "        [8.0630e-04, 8.0630e-04, 8.0630e-04, 1.5449e-03, 8.0630e-04, 8.0630e-04,\n",
      "         9.9442e-01],\n",
      "        [4.8874e-01, 1.1894e-02, 1.1894e-02, 1.9137e-02, 1.1894e-02, 4.4454e-01,\n",
      "         1.1894e-02],\n",
      "        [7.9919e-01, 1.6982e-02, 1.6982e-02, 1.1590e-01, 1.6982e-02, 1.6982e-02,\n",
      "         1.6982e-02],\n",
      "        [8.9553e-03, 8.9553e-03, 8.9553e-03, 9.3486e-01, 2.0364e-02, 8.9553e-03,\n",
      "         8.9553e-03],\n",
      "        [9.9859e-01, 2.6056e-04, 2.2582e-04, 2.4799e-04, 2.2582e-04, 2.2582e-04,\n",
      "         2.2582e-04],\n",
      "        [1.3335e-03, 1.6537e-03, 1.3335e-03, 1.4964e-03, 1.3335e-03, 1.3335e-03,\n",
      "         9.9152e-01],\n",
      "        [1.7457e-02, 8.9526e-01, 1.7457e-02, 1.7457e-02, 1.7457e-02, 1.7457e-02,\n",
      "         1.7457e-02],\n",
      "        [2.1734e-02, 2.1734e-02, 2.1734e-02, 2.8717e-01, 2.1734e-02, 6.0416e-01,\n",
      "         2.1734e-02],\n",
      "        [1.4103e-03, 1.4103e-03, 1.4103e-03, 9.9098e-01, 1.4103e-03, 1.4103e-03,\n",
      "         1.9733e-03],\n",
      "        [5.6016e-03, 5.6016e-03, 5.6016e-03, 5.6016e-03, 5.8335e-03, 5.6016e-03,\n",
      "         9.6616e-01],\n",
      "        [2.1433e-03, 2.1433e-03, 2.1433e-03, 2.1433e-03, 3.4928e-03, 9.8561e-01,\n",
      "         2.3284e-03],\n",
      "        [5.8957e-01, 3.2601e-02, 2.9512e-02, 2.9512e-02, 2.9512e-02, 2.5074e-01,\n",
      "         3.8551e-02],\n",
      "        [1.7100e-02, 1.9549e-02, 1.7100e-02, 1.7100e-02, 8.8250e-01, 1.7100e-02,\n",
      "         2.9552e-02],\n",
      "        [1.2893e-02, 1.2893e-02, 1.2893e-02, 9.1988e-01, 1.2893e-02, 1.5660e-02,\n",
      "         1.2893e-02],\n",
      "        [1.5617e-02, 1.5617e-02, 9.0429e-01, 1.5617e-02, 1.7621e-02, 1.5617e-02,\n",
      "         1.5617e-02],\n",
      "        [1.7832e-02, 1.7832e-02, 2.5859e-02, 1.7832e-02, 8.7919e-01, 2.3618e-02,\n",
      "         1.7832e-02],\n",
      "        [9.8499e-01, 2.2630e-03, 2.4562e-03, 2.0692e-03, 2.0692e-03, 4.0833e-03,\n",
      "         2.0692e-03],\n",
      "        [1.5053e-02, 1.5053e-02, 1.5053e-02, 1.7569e-02, 8.4153e-01, 1.5053e-02,\n",
      "         8.0692e-02],\n",
      "        [3.5609e-03, 3.5609e-03, 3.5609e-03, 3.5609e-03, 3.5609e-03, 6.8479e-03,\n",
      "         9.7535e-01],\n",
      "        [5.4547e-03, 9.4510e-01, 1.9161e-02, 5.4547e-03, 5.4547e-03, 5.4547e-03,\n",
      "         1.3924e-02],\n",
      "        [2.1580e-02, 8.7052e-01, 2.1580e-02, 2.1580e-02, 2.1580e-02, 2.1580e-02,\n",
      "         2.1580e-02],\n",
      "        [9.8021e-03, 9.8021e-03, 9.3288e-01, 9.8021e-03, 9.8021e-03, 1.8106e-02,\n",
      "         9.8021e-03],\n",
      "        [2.1628e-02, 2.1628e-02, 2.9722e-02, 8.6214e-01, 2.1628e-02, 2.1628e-02,\n",
      "         2.1628e-02],\n",
      "        [8.7136e-01, 4.2258e-02, 1.6693e-02, 1.6693e-02, 1.9612e-02, 1.6693e-02,\n",
      "         1.6693e-02],\n",
      "        [1.4639e-03, 1.4639e-03, 1.4639e-03, 1.4639e-03, 1.4639e-03, 9.9122e-01,\n",
      "         1.4639e-03],\n",
      "        [4.9942e-03, 4.9942e-03, 9.6115e-01, 4.9942e-03, 6.0226e-03, 5.0050e-03,\n",
      "         1.2840e-02],\n",
      "        [3.6032e-03, 3.6032e-03, 3.6032e-03, 9.6795e-01, 1.3932e-02, 3.7041e-03,\n",
      "         3.6032e-03],\n",
      "        [2.8709e-02, 8.2775e-01, 2.8709e-02, 2.8709e-02, 2.8709e-02, 2.8709e-02,\n",
      "         2.8709e-02],\n",
      "        [8.1309e-04, 8.1309e-04, 8.1309e-04, 8.1309e-04, 2.6063e-03, 9.9333e-01,\n",
      "         8.1309e-04],\n",
      "        [1.7536e-02, 1.7536e-02, 8.9477e-01, 1.7536e-02, 1.7536e-02, 1.7548e-02,\n",
      "         1.7536e-02],\n",
      "        [1.3463e-02, 1.3463e-02, 2.4373e-02, 1.3463e-02, 8.7843e-01, 4.3349e-02,\n",
      "         1.3463e-02],\n",
      "        [8.4801e-01, 3.6399e-02, 2.2760e-02, 2.2760e-02, 2.2760e-02, 2.4548e-02,\n",
      "         2.2760e-02],\n",
      "        [2.7790e-03, 9.8108e-01, 5.0262e-03, 2.7790e-03, 2.7790e-03, 2.7790e-03,\n",
      "         2.7790e-03],\n",
      "        [1.9002e-02, 5.6641e-02, 1.9002e-02, 1.9002e-02, 8.4835e-01, 1.9002e-02,\n",
      "         1.9002e-02],\n",
      "        [6.0781e-03, 9.6353e-01, 6.0781e-03, 6.0781e-03, 6.0781e-03, 6.0781e-03,\n",
      "         6.0781e-03],\n",
      "        [9.9771e-01, 2.5246e-04, 2.5246e-04, 7.4700e-04, 2.5246e-04, 5.3779e-04,\n",
      "         2.5246e-04],\n",
      "        [2.4704e-03, 7.7552e-04, 9.8112e-01, 1.7692e-03, 7.7552e-04, 1.2315e-02,\n",
      "         7.7552e-04],\n",
      "        [4.6226e-03, 9.7226e-01, 4.6226e-03, 4.6226e-03, 4.6226e-03, 4.6226e-03,\n",
      "         4.6226e-03],\n",
      "        [1.7884e-02, 8.9269e-01, 1.7884e-02, 1.7884e-02, 1.7884e-02, 1.7884e-02,\n",
      "         1.7884e-02],\n",
      "        [1.0001e-03, 1.2335e-03, 9.9377e-01, 1.0001e-03, 1.0001e-03, 1.0001e-03,\n",
      "         1.0001e-03],\n",
      "        [4.4487e-03, 4.4487e-03, 4.4487e-03, 4.4487e-03, 9.4433e-01, 3.3427e-02,\n",
      "         4.4487e-03],\n",
      "        [8.7534e-01, 1.8879e-02, 1.8879e-02, 1.8879e-02, 1.8879e-02, 1.8879e-02,\n",
      "         3.0268e-02],\n",
      "        [2.6755e-03, 2.6755e-03, 9.8217e-01, 4.4506e-03, 2.6755e-03, 2.6755e-03,\n",
      "         2.6755e-03],\n",
      "        [2.7715e-09, 2.7715e-09, 1.0000e+00, 2.7715e-09, 2.7715e-09, 2.7715e-09,\n",
      "         2.7715e-09],\n",
      "        [1.6410e-05, 1.6410e-05, 1.6410e-05, 3.6451e-05, 1.6410e-05, 9.9988e-01,\n",
      "         1.6410e-05],\n",
      "        [3.9465e-04, 3.9465e-04, 3.9465e-04, 3.9465e-04, 4.2627e-04, 9.9760e-01,\n",
      "         3.9465e-04],\n",
      "        [5.8219e-02, 8.5904e-02, 6.0435e-02, 7.6254e-02, 6.1888e-02, 5.8219e-02,\n",
      "         5.9908e-01],\n",
      "        [7.4468e-03, 7.4468e-03, 7.4468e-03, 9.0863e-03, 9.2982e-01, 3.1304e-02,\n",
      "         7.4468e-03],\n",
      "        [1.0117e-02, 9.3930e-01, 1.0117e-02, 1.0117e-02, 1.0117e-02, 1.0117e-02,\n",
      "         1.0117e-02],\n",
      "        [6.0419e-05, 6.0419e-05, 6.0419e-05, 6.0419e-05, 6.0419e-05, 6.0419e-05,\n",
      "         9.9964e-01],\n",
      "        [3.9368e-03, 7.3419e-03, 3.9368e-03, 3.9368e-03, 3.9368e-03, 3.9368e-03,\n",
      "         9.7297e-01],\n",
      "        [2.6064e-02, 2.6064e-02, 2.6231e-02, 5.9612e-02, 2.6064e-02, 8.0990e-01,\n",
      "         2.6064e-02],\n",
      "        [2.6244e-03, 2.6244e-03, 9.7918e-01, 7.7003e-03, 2.6244e-03, 2.6244e-03,\n",
      "         2.6244e-03],\n",
      "        [1.6385e-02, 9.0169e-01, 1.6385e-02, 1.6385e-02, 1.6385e-02, 1.6385e-02,\n",
      "         1.6385e-02],\n",
      "        [4.9686e-05, 1.3701e-04, 9.9928e-01, 4.9686e-05, 4.9686e-05, 2.8183e-04,\n",
      "         1.5406e-04],\n",
      "        [4.9196e-04, 4.9196e-04, 4.9196e-04, 4.9196e-04, 9.9705e-01, 4.9196e-04,\n",
      "         4.9196e-04],\n",
      "        [7.7689e-03, 7.7689e-03, 7.7689e-03, 7.7689e-03, 9.2550e-01, 3.5655e-02,\n",
      "         7.7689e-03],\n",
      "        [3.3685e-03, 3.3685e-03, 5.3887e-03, 3.3685e-03, 3.3685e-03, 6.2060e-03,\n",
      "         9.7493e-01],\n",
      "        [1.9888e-03, 1.9888e-03, 5.0290e-03, 1.9888e-03, 1.9888e-03, 9.8132e-01,\n",
      "         5.6945e-03],\n",
      "        [8.3892e-04, 8.3892e-04, 8.3892e-04, 1.2523e-03, 8.3892e-04, 8.3892e-04,\n",
      "         9.9455e-01],\n",
      "        [9.9998e-01, 1.2772e-06, 1.3173e-05, 3.0514e-06, 1.2772e-06, 1.2772e-06,\n",
      "         1.2772e-06],\n",
      "        [6.1975e-03, 6.1975e-03, 9.4574e-01, 1.4809e-02, 6.1975e-03, 6.1975e-03,\n",
      "         1.4664e-02],\n",
      "        [2.3029e-03, 2.3029e-03, 2.7435e-03, 2.3029e-03, 2.3029e-03, 2.3029e-03,\n",
      "         9.8574e-01],\n",
      "        [4.1182e-03, 4.1182e-03, 4.1182e-03, 4.1182e-03, 9.2995e-01, 4.9456e-02,\n",
      "         4.1182e-03],\n",
      "        [4.5590e-02, 2.4148e-03, 8.0215e-03, 3.2744e-03, 2.4148e-03, 9.3587e-01,\n",
      "         2.4148e-03],\n",
      "        [5.5873e-04, 5.5873e-04, 6.8132e-04, 9.9653e-01, 5.5873e-04, 5.5873e-04,\n",
      "         5.5873e-04],\n",
      "        [8.6015e-03, 8.6015e-03, 9.0604e-01, 5.0952e-02, 8.6015e-03, 8.6015e-03,\n",
      "         8.6015e-03],\n",
      "        [3.5331e-03, 5.0482e-03, 9.7638e-01, 4.4402e-03, 3.5331e-03, 3.5331e-03,\n",
      "         3.5331e-03],\n",
      "        [1.6649e-04, 2.8176e-05, 2.8176e-05, 2.8176e-05, 2.8176e-05, 9.9969e-01,\n",
      "         2.8176e-05],\n",
      "        [9.6568e-01, 5.0730e-03, 4.7285e-03, 3.6922e-03, 3.6922e-03, 1.3442e-02,\n",
      "         3.6922e-03],\n",
      "        [7.5446e-05, 9.9925e-01, 3.4905e-04, 7.5446e-05, 7.5446e-05, 7.5446e-05,\n",
      "         1.0101e-04],\n",
      "        [2.1749e-03, 2.1749e-03, 2.1749e-03, 3.2667e-03, 2.1749e-03, 2.1749e-03,\n",
      "         9.8586e-01],\n",
      "        [9.3868e-01, 1.6214e-03, 4.5577e-02, 3.1705e-03, 1.6214e-03, 7.7106e-03,\n",
      "         1.6214e-03],\n",
      "        [1.0000e+00, 1.3409e-11, 1.3409e-11, 5.4856e-11, 1.3409e-11, 1.3409e-11,\n",
      "         1.3409e-11],\n",
      "        [1.2752e-02, 1.2752e-02, 1.1685e-01, 7.9421e-01, 1.2752e-02, 1.2752e-02,\n",
      "         3.7934e-02],\n",
      "        [2.1328e-03, 2.8163e-03, 2.1328e-03, 2.1328e-03, 9.2160e-01, 2.1328e-03,\n",
      "         6.7057e-02],\n",
      "        [4.7124e-04, 4.7124e-04, 4.7124e-04, 4.7124e-04, 3.0206e-03, 4.7124e-04,\n",
      "         9.9462e-01],\n",
      "        [2.3564e-04, 9.9810e-01, 2.3564e-04, 2.3564e-04, 2.3564e-04, 7.1896e-04,\n",
      "         2.3564e-04],\n",
      "        [2.2879e-06, 2.2879e-06, 2.2879e-06, 2.2879e-06, 7.5202e-06, 9.9998e-01,\n",
      "         2.2879e-06],\n",
      "        [1.9519e-04, 1.9519e-04, 1.9519e-04, 1.9519e-04, 1.9519e-04, 9.9883e-01,\n",
      "         1.9519e-04],\n",
      "        [8.3253e-01, 2.8183e-02, 1.5632e-02, 1.5632e-02, 1.5632e-02, 7.6762e-02,\n",
      "         1.5632e-02],\n",
      "        [2.9549e-03, 9.7857e-01, 2.9549e-03, 2.9549e-03, 2.9549e-03, 2.9549e-03,\n",
      "         6.6508e-03],\n",
      "        [8.1518e-05, 8.1518e-05, 8.1518e-05, 9.9951e-01, 8.1518e-05, 8.1518e-05,\n",
      "         8.1518e-05],\n",
      "        [8.4320e-04, 1.1735e-03, 8.4320e-04, 1.0710e-03, 1.8395e-03, 9.9339e-01,\n",
      "         8.4320e-04],\n",
      "        [4.6814e-03, 5.8231e-03, 1.2779e-02, 9.6118e-01, 4.6814e-03, 4.6814e-03,\n",
      "         6.1722e-03],\n",
      "        [3.6096e-04, 3.6096e-04, 9.9783e-01, 3.6096e-04, 3.6096e-04, 3.6096e-04,\n",
      "         3.6096e-04],\n",
      "        [3.6096e-04, 3.6096e-04, 9.9783e-01, 3.6096e-04, 3.6096e-04, 3.6096e-04,\n",
      "         3.6096e-04],\n",
      "        [7.0016e-03, 7.0016e-03, 5.6908e-02, 8.2565e-01, 7.0016e-03, 8.9434e-02,\n",
      "         7.0016e-03],\n",
      "        [1.3473e-04, 1.3473e-04, 1.3473e-04, 1.5586e-04, 1.3473e-04, 1.3473e-04,\n",
      "         9.9917e-01]], grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1.]])\n",
      "tensor([[0.0167, 0.0217, 0.8633,  ..., 0.0157, 0.0157, 0.0512],\n",
      "        [0.0534, 0.0534, 0.2598,  ..., 0.0534, 0.3463, 0.1395],\n",
      "        [0.0691, 0.0691, 0.0691,  ..., 0.3072, 0.2800, 0.1356],\n",
      "        ...,\n",
      "        [0.0403, 0.0251, 0.0251,  ..., 0.1685, 0.6907, 0.0251],\n",
      "        [0.6629, 0.0173, 0.0414,  ..., 0.0173, 0.0903, 0.1536],\n",
      "        [0.0052, 0.0137, 0.0059,  ..., 0.0052, 0.0052, 0.0052]],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        ...,\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.]])\n",
      "tensor([[0.0547, 0.0547, 0.0547,  ..., 0.0605, 0.3775, 0.0547],\n",
      "        [0.0252, 0.0252, 0.0370,  ..., 0.0252, 0.8243, 0.0252],\n",
      "        [0.0963, 0.1903, 0.0963,  ..., 0.1059, 0.0963, 0.1856],\n",
      "        ...,\n",
      "        [0.0014, 0.1636, 0.0014,  ..., 0.8294, 0.0014, 0.0014],\n",
      "        [0.1493, 0.2518, 0.0630,  ..., 0.0630, 0.0630, 0.2900],\n",
      "        [0.0220, 0.1375, 0.3685,  ..., 0.0220, 0.0220, 0.0220]],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.]])\n",
      "Train Accuracy: 0.9786\n",
      "Val Accuracy: 0.2804\n",
      "Test Accuracy: 0.3395\n"
     ]
    },
    {
     "data": {
      "text/plain": "GCN(\n  (conv1): GCNConv(1433, 64)\n  (conv2): GCNConv(64, 7)\n)"
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patience = 7\n",
    "best_val_loss = 10000\n",
    "best_model = model\n",
    "\n",
    "for epoch in range(1, 101):\n",
    "    if patience == 0:\n",
    "        break\n",
    "    loss, val_loss = train()\n",
    "    if val_loss <= best_val_loss:\n",
    "        patience = 10\n",
    "        best_val_loss = val_loss\n",
    "        best_model = model\n",
    "    else:\n",
    "        patience -= 1\n",
    "\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "\n",
    "\n",
    "train_acc = test(best_model, 'train')\n",
    "val_acc = test(best_model, 'val')\n",
    "test_acc = test(best_model)\n",
    "\n",
    "print(f'Train Accuracy: {train_acc:.4f}')\n",
    "print(f'Val Accuracy: {val_acc:.4f}')\n",
    "print(f'Test Accuracy: {test_acc:.4f}')\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# out = model(data.x, data.edge_index)\n",
    "# visualize(out, color=data.y)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
